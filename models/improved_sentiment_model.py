# improved_sentiment_model.py
import pandas as pd
import numpy as np
import pickle
import joblib
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from datetime import datetime
import os
import logging
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# T√©l√©charger les ressources NLTK n√©cessaires
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedSentimentModelTrainer:
    def __init__(self, model_name="retail_sentiment_v1"):
        self.model_name = model_name
        self.model = None
        self.vectorizer = None
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
        # Cr√©er le dossier des mod√®les s'il n'existe pas
        os.makedirs('/opt/airflow/models', exist_ok=True)
    
    def preprocess_text(self, text):
        """Pr√©processer le texte pour l'analyse de sentiment"""
        if pd.isna(text) or text is None:
            return ""
        
        # Convertir en string si ce n'est pas d√©j√† le cas
        text = str(text)
        
        # Convertir en minuscules
        text = text.lower()
        
        # Supprimer les URLs, mentions, hashtags
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Supprimer les caract√®res sp√©ciaux et chiffres
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # Supprimer les espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Si le texte est vide apr√®s nettoyage
        if not text:
            return ""
        
        try:
            # Tokenisation
            tokens = word_tokenize(text)
            
            # Supprimer les mots vides et lemmatisation
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens 
                     if token not in self.stop_words and len(token) > 2]
            
            return ' '.join(tokens)
        except Exception as e:
            logger.warning(f"Erreur lors du preprocessing: {e}")
            return text
    
    def load_sentiment140_data(self, data_path="data/training.1600000.processed.noemoticon.csv", sample_size=200000):
        """Charger les donn√©es Sentiment140"""
        logger.info(f"üìÇ Chargement des donn√©es depuis {data_path}")
        
        # Colonnes du dataset Sentiment140
        columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']
        
        try:
            # V√©rifier si le fichier existe
            if not os.path.exists(data_path):
                logger.error(f"‚ùå Fichier non trouv√©: {data_path}")
                logger.info("üîÑ Cr√©ation de donn√©es synth√©tiques...")
                return self._create_synthetic_data()
            
            # Charger un √©chantillon des donn√©es
            data = pd.read_csv(data_path, encoding='latin-1', names=columns, 
                             nrows=sample_size, header=None)
            
            # Mapper les sentiments (0=n√©gatif, 4=positif) - pas de neutre dans Sentiment140
            sentiment_mapping = {0: 'negative', 4: 'positive'}
            data['sentiment'] = data['sentiment'].map(sentiment_mapping)
            
            # Supprimer les lignes avec sentiment manquant
            data = data.dropna(subset=['sentiment', 'text'])
            
            # Filtrer les textes vides
            data = data[data['text'].str.strip() != '']
            
            logger.info(f"‚úÖ Donn√©es charg√©es: {len(data)} tweets")
            logger.info(f"Distribution des sentiments:\n{data['sentiment'].value_counts()}")
            
            # Ajouter des donn√©es neutres synth√©tiques pour √©quilibrer
            neutral_data = self._create_neutral_data(len(data) // 10)  # 10% de neutres
            
            final_data = pd.concat([data[['text', 'sentiment']], neutral_data], ignore_index=True)
            
            logger.info(f"‚úÖ Donn√©es finales avec neutres: {len(final_data)} tweets")
            logger.info(f"Distribution finale:\n{final_data['sentiment'].value_counts()}")
            
            return final_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement: {e}")
            # Cr√©er des donn√©es synth√©tiques si le fichier n'existe pas
            return self._create_synthetic_data()
    
    def _create_neutral_data(self, size):
        """Cr√©er des donn√©es neutres synth√©tiques"""
        neutral_texts = [
            "Product arrived on time as expected",
            "Standard quality for the price point",
            "Nothing special but does the job",
            "Average experience neither good nor bad",
            "Product is okay meets basic requirements",
            "Item works as described in listing",
            "Delivery was on schedule no issues",
            "Product matches the description given",
            "Regular customer service experience",
            "Standard packaging and shipping process",
            "Item received in expected condition",
            "Process was straightforward and simple",
            "No complaints about the transaction",
            "Everything went according to plan",
            "Product functions as it should",
            "Typical online shopping experience",
            "Item arrived in reasonable timeframe",
            "Customer service was adequate",
            "Product quality meets expectations",
            "No major issues with the order"
        ]
        
        # R√©pliquer pour atteindre la taille demand√©e
        multiplier = (size // len(neutral_texts)) + 1
        expanded_texts = (neutral_texts * multiplier)[:size]
        
        return pd.DataFrame({
            'text': expanded_texts,
            'sentiment': ['neutral'] * len(expanded_texts)
        })
    
    def _create_synthetic_data(self):
        """Cr√©er des donn√©es synth√©tiques pour l'entra√Ænement"""
        logger.info("üîÑ Cr√©ation de donn√©es synth√©tiques...")
        
        synthetic_data = [
            # Tweets positifs
            ("I love this product amazing quality and fast shipping", "positive"),
            ("Great service at this store highly recommend to everyone", "positive"),
            ("Excellent customer experience will definitely buy again", "positive"),
            ("Best purchase I have made this year absolutely perfect", "positive"),
            ("Outstanding quality and incredibly fast delivery service", "positive"),
            ("Amazing product quality exceeded all my expectations completely", "positive"),
            ("Fantastic customer support team helped me resolve issues quickly", "positive"),
            ("Love the design and functionality works perfectly every time", "positive"),
            ("Incredible value for money definitely worth every penny spent", "positive"),
            ("Superb quality construction and attention to detail evident", "positive"),
            
            # Tweets n√©gatifs
            ("Terrible product quality complete waste of money and time", "negative"),
            ("Poor customer service very disappointed with entire experience", "negative"),
            ("Product broke after one day awful quality control", "negative"),
            ("Never shopping here again horrible experience overall", "negative"),
            ("Completely unsatisfied with this purchase regret buying", "negative"),
            ("Worst customer service ever encountered extremely frustrating", "negative"),
            ("Product does not work as advertised misleading description", "negative"),
            ("Overpriced for such poor quality materials and construction", "negative"),
            ("Delivery was delayed multiple times terrible communication", "negative"),
            ("Product arrived damaged and return process was complicated", "negative"),
            
            # Tweets neutres
            ("Product arrived on time as expected nothing special", "neutral"),
            ("Standard quality for the price point meets basic needs", "neutral"),
            ("Nothing special but does the job adequately", "neutral"),
            ("Average experience neither particularly good nor bad", "neutral"),
            ("Product is okay meets basic requirements sufficiently", "neutral"),
            ("Item works as described in the listing", "neutral"),
            ("Delivery was on schedule no major issues", "neutral"),
            ("Product matches the description provided by seller", "neutral"),
            ("Regular customer service experience nothing noteworthy", "neutral"),
            ("Standard packaging and shipping process typical", "neutral"),
        ]
        
        # R√©pliquer les donn√©es pour avoir plus d'√©chantillons
        synthetic_data = synthetic_data * 2000  # 60,000 √©chantillons
        
        return pd.DataFrame(synthetic_data, columns=['text', 'sentiment'])
    
    def train_model(self, data, test_size=0.2):
        """Entra√Æner le mod√®le de sentiment"""
        logger.info("üöÄ D√©but de l'entra√Ænement du mod√®le")
        
        # Pr√©processer les textes
        logger.info("üîÑ Pr√©processing des textes...")
        data['processed_text'] = data['text'].apply(self.preprocess_text)
        
        # Supprimer les textes vides apr√®s preprocessing
        data = data[data['processed_text'].str.strip() != '']
        
        if len(data) == 0:
            raise ValueError("Aucun texte valide apr√®s preprocessing")
        
        # S√©parer les donn√©es
        X = data['processed_text']
        y = data['sentiment']
        
        logger.info(f"üìä Donn√©es d'entra√Ænement: {len(X)} √©chantillons")
        logger.info(f"üìä Distribution: {y.value_counts().to_dict()}")
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # Cr√©er le pipeline avec param√®tres optimis√©s
        self.model = Pipeline([
            ('tfidf', TfidfVectorizer(
                max_features=15000,
                ngram_range=(1, 2),
                min_df=3,
                max_df=0.9,
                stop_words='english',
                lowercase=True,
                token_pattern=r'\b\w+\b'
            )),
            ('classifier', LogisticRegression(
                random_state=42,
                max_iter=2000,
                class_weight='balanced',
                C=1.0,
                solver='liblinear'
            ))
        ])
        
        # Entra√Æner le mod√®le
        logger.info("üîÑ Entra√Ænement en cours...")
        self.model.fit(X_train, y_train)
        
        # √âvaluer le mod√®le
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        logger.info(f"‚úÖ Entra√Ænement termin√© - Accuracy: {accuracy:.4f}")
        logger.info("\nüìä Rapport de classification:")
        logger.info(classification_report(y_test, y_pred))
        
        return accuracy, y_test, y_pred
    
    def predict_sentiment(self, text):
        """Pr√©dire le sentiment d'un texte - m√©thode compatible avec sentiment_processor.py"""
        if self.model is None:
            raise ValueError("Mod√®le non entra√Æn√©. Chargez un mod√®le ou entra√Ænez-en un.")
        
        if not text or str(text).strip() == "":
            return {"sentiment": "neutral", "confidence": 0.5}
        
        try:
            processed_text = self.preprocess_text(text)
            if not processed_text:
                return {"sentiment": "neutral", "confidence": 0.5}
            
            prediction = self.model.predict([processed_text])[0]
            probabilities = self.model.predict_proba([processed_text])[0]
            confidence = max(probabilities)
            
            return {
                'sentiment': prediction,
                'confidence': float(confidence)
            }
        except Exception as e:
            logger.error(f"Erreur lors de la pr√©diction: {e}")
            return {"sentiment": "neutral", "confidence": 0.5}
    
    def predict_sentiment_long_text(self, text):
        """Pr√©dire le sentiment d'un texte (m√©thode √©tendue)"""
        result = self.predict_sentiment(text)
        
        if self.model is not None:
            try:
                processed_text = self.preprocess_text(text)
                if processed_text:
                    probabilities = self.model.predict_proba([processed_text])[0]
                    result['probabilities'] = dict(zip(self.model.classes_, probabilities))
            except Exception as e:
                logger.warning(f"Impossible d'obtenir les probabilit√©s: {e}")
        
        return result
    
    def save_model(self, path):
        """Sauvegarder le mod√®le avec gestion d'erreurs am√©lior√©e"""
        if self.model is None:
            raise ValueError("Aucun mod√®le √† sauvegarder")
        
        try:
            # Cr√©er le dossier si n√©cessaire
            os.makedirs(os.path.dirname(path), exist_ok=True)
            
            model_data = {
                'model': self.model,
                'model_name': self.model_name,
                'created_at': datetime.now().isoformat(),
                'version': '2.0.0'
            }
            
            # Sauvegarder avec pickle
            with open(path, 'wb') as f:
                pickle.dump(model_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # V√©rifier que le fichier a √©t√© cr√©√© correctement
            if os.path.exists(path) and os.path.getsize(path) > 0:
                logger.info(f"üíæ Mod√®le sauvegard√© avec succ√®s: {path}")
                logger.info(f"üìè Taille du fichier: {os.path.getsize(path)} bytes")
            else:
                raise Exception("Fichier non cr√©√© ou vide")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la sauvegarde: {e}")
            raise
    
    def load_model(self, path):
        """Charger un mod√®le sauvegard√© avec gestion d'erreurs am√©lior√©e"""
        try:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Fichier mod√®le non trouv√©: {path}")
            
            if os.path.getsize(path) == 0:
                raise ValueError(f"Fichier mod√®le vide: {path}")
            
            logger.info(f"üìÇ Chargement du mod√®le depuis: {path}")
            logger.info(f"üìè Taille du fichier: {os.path.getsize(path)} bytes")
            
            with open(path, 'rb') as f:
                model_data = pickle.load(f)
            
            if not isinstance(model_data, dict):
                raise ValueError("Format de mod√®le invalide")
            
            if 'model' not in model_data:
                raise ValueError("Mod√®le manquant dans les donn√©es")
            
            self.model = model_data['model']
            self.model_name = model_data.get('model_name', 'loaded_model')
            
            logger.info(f"‚úÖ Mod√®le charg√© avec succ√®s: {self.model_name}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise

class SentimentModelPipeline:
    def __init__(self, model_name="retail_sentiment_v2"):
        self.model_name = model_name
        self.trainer = ImprovedSentimentModelTrainer(model_name)
        self.model_version = "2.0.0"
        
        # Initialiser MLflow (optionnel)
        try:
            mlflow.set_experiment("retail_sentiment_analysis")
        except Exception as e:
            logger.warning(f"MLflow non disponible: {e}")
    
    def train_and_save_model(self, data_path="data/training.1600000.processed.noemoticon.csv"):
        """Entra√Æner et sauvegarder le mod√®le final"""
        logger.info("üöÄ D√©but de l'entra√Ænement du mod√®le de sentiment")
        
        # Charger et pr√©processer les donn√©es
        data = self.trainer.load_sentiment140_data(data_path)
        
        # Entra√Æner le mod√®le
        accuracy, y_test, y_pred = self.trainer.train_model(data, test_size=0.2)
        
        # Sauvegarder le mod√®le
        model_path = f"/opt/airflow/models/{self.model_name}_v{self.model_version}.pkl"
        self.trainer.save_model(model_path)
        
        # Sauvegarder les m√©triques
        self._save_model_metrics(accuracy, y_test, y_pred)
        
        logger.info(f"‚úÖ Mod√®le sauvegard√©: {model_path}")
        logger.info(f"üìä Accuracy finale: {accuracy:.4f}")
        
        return model_path, accuracy
    
    def _save_model_metrics(self, accuracy, y_test, y_pred):
        """Sauvegarder les m√©triques dans MLflow"""
        try:
            with mlflow.start_run():
                mlflow.log_metric("accuracy", accuracy)
                mlflow.log_param("model_version", self.model_version)
                mlflow.log_param("model_name", self.model_name)
                mlflow.log_param("training_date", datetime.now().isoformat())
                
                # Log classification report
                report = classification_report(y_test, y_pred, output_dict=True)
                for label, metrics in report.items():
                    if isinstance(metrics, dict):
                        for metric, value in metrics.items():
                            mlflow.log_metric(f"{label}_{metric}", value)
                
                # Sauvegarder le mod√®le dans MLflow
                mlflow.sklearn.log_model(self.trainer.model, "model")
                
        except Exception as e:
            logger.warning(f"Impossible de sauvegarder dans MLflow: {e}")
    
    def test_model_on_sample_tweets(self, model_path):
        """Tester le mod√®le sur des tweets d'exemple"""
        # Charger le mod√®le
        trainer = ImprovedSentimentModelTrainer()
        trainer.load_model(model_path)
        
        # Tweets de test
        test_tweets = [
            "Just got my iPhone 13 from the Apple Store in NYC - amazing device and great service!",
            "Terrible experience at Best Buy today, long wait times and completely unhelpful staff",
            "Thinking about buying the new Samsung Galaxy, any recommendations from users?",
            "Love my new Nike shoes, super comfortable for running and great design!",
            "Had serious issues with my Sony headphones, sound quality is really poor",
            "Great customer service at the Miami store, very helpful and friendly staff",
            "Product arrived on time and works exactly as expected, nothing special",
            "Worst purchase ever made, completely disappointed with the quality and service"
        ]
        
        logger.info("üß™ Test du mod√®le sur des tweets d'exemple:")
        for i, tweet in enumerate(test_tweets, 1):
            result = trainer.predict_sentiment_long_text(tweet)
            logger.info(f"{i}. Tweet: {tweet[:60]}...")
            logger.info(f"   Sentiment: {result['sentiment']} (confiance: {result['confidence']:.3f})")
            logger.info("-" * 70)

if __name__ == "__main__":
    # Entra√Æner le mod√®le final
    pipeline = SentimentModelPipeline()
    model_path, accuracy = pipeline.train_and_save_model()
    
    # Tester le mod√®le
    pipeline.test_model_on_sample_tweets(model_path)